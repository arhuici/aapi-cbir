{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autores:\n",
    "- Alejandro Pastor Membrado\n",
    "- Ángel Romero Huici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías necesarias para procesar imágenes, extraer características y gestionar índices de búsqueda con FAISS. También configura el entorno para evitar conflictos de librerías con OpenMP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import cv2\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from faiss import read_index, write_index\n",
    "from skimage.io import imread_collection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights, ResNet152_Weights, EfficientNet_B0_Weights\n",
    "\n",
    "#Soluciona un posible problema relacionado con duplicados de bibliotecas en sistemas que usan OpenMP\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjunto de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto original de imágenes se ha obtenido de https://www.kaggle.com/datasets/elkamel/corel-images?resource=download y consta de 1000 imágenes organizadas en 10 clases. De estas, se utilizarán 900 imágenes para entrenamiento (90 por clase) para extraer las características del sistema, y las 100 restantes (10 por clase) se destinarán a pruebas para evaluar el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de directorios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se procesan y almacenan las imágenes obtenidas del conjunto de datos descargado de Kaggle en dos directorios separados. Las 900 imágenes de entrenamiento se guardan en la carpeta images y se utilizan para extraer las características que conformarán la base de datos del modelo. Por otro lado, las imágenes de prueba se almacenan en la carpeta queries y se emplean para evaluar el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base de datos de imágenes (images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La carpeta images ya existe. No se realizaron cambios.\n"
     ]
    }
   ],
   "source": [
    "col_dir = 'dataset/training_set/**/*.jpg'\n",
    "image_paths = glob.glob(col_dir, recursive=True)\n",
    "\n",
    "#El código únicamente se ejecuta si la carpeta images no existe\n",
    "if not os.path.exists('images'):\n",
    "    os.mkdir('images')\n",
    "    col = imread_collection(image_paths)\n",
    "    for i in range(len(col)):\n",
    "        im = Image.fromarray(col[i])\n",
    "        im.save(f\"images/{i}.jpg\")\n",
    "    print(\"Imágenes almacenadas en la carpeta images.\")\n",
    "else:\n",
    "    print(\"La carpeta images ya existe. No se realizaron cambios.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imágenes de búsqueda (queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La carpeta queries ya existe. No se realizaron cambios.\n"
     ]
    }
   ],
   "source": [
    "col_test_dir = 'dataset/test_set/**/*.jpg'\n",
    "test_image_paths = glob.glob(col_test_dir, recursive=True)\n",
    "\n",
    "#El código únicamente se ejecuta si la carpeta queries no existe\n",
    "if not os.path.exists('queries'):\n",
    "    os.mkdir('queries')\n",
    "    col_test = imread_collection(test_image_paths)\n",
    "    for i in range(len(col_test)):\n",
    "        im = Image.fromarray(col_test[i])\n",
    "        im.save(f\"queries/{i}.jpg\")\n",
    "    print(\"Imágenes de prueba almacenadas en la carpeta queries.\")\n",
    "else:\n",
    "    print(\"La carpeta queries ya existe. No se realizaron cambios.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cargan las imágenes almacenadas en el paso anterior y se guardan en una variable de tipo ImageCollection de skimage, un tipo de objeto diseñado para gestionar y manipular conjuntos de imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de imágenes base de datos: 900\n"
     ]
    }
   ],
   "source": [
    "col_dir = 'images/*.jpg'\n",
    "image_paths = glob.glob(col_dir, recursive=True)\n",
    "col = imread_collection(image_paths)\n",
    "\n",
    "print(f\"Cantidad de imágenes base de datos: {len(col)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de imágenes queries: 100\n"
     ]
    }
   ],
   "source": [
    "col_test_dir = 'queries/*.jpg'\n",
    "test_image_paths = glob.glob(col_test_dir, recursive=True)\n",
    "col_test = imread_collection(test_image_paths)\n",
    "\n",
    "print(f\"Cantidad de imágenes queries: {len(col_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cargan y configuran tres modelos de redes neuronales preentrenados de PyTorch para su uso como extractores de características de imágenes. Todos los modelos han sido entrenados previamente con pesos de ImageNet, y el procedimiento es similar en cada caso: se elimina o desactiva la última capa, que se utilizaría para tareas de clasificación, permitiendo que el modelo devuelva únicamente las características extraídas. Los modelos utilizados son:\n",
    "* ResNet-152\n",
    "* EfficientNet\n",
    "* Inception V3\n",
    "\n",
    "En el caso de ResNet-152 y EfficientNet, se elimina la última capa y se desactiva el ajuste de pesos para asegurar que solo se extraigan características. En el caso de Inception V3, la capa de clasificación se reemplaza por una capa de identidad y el modelo se coloca en modo de evaluación para asegurar un funcionamiento correcto durante la extracción de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inception3(\n",
       "  (Conv2d_1a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2b_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Conv2d_3b_1x1): BasicConv2d(\n",
       "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_4a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Mixed_5b): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5c): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5d): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6a): InceptionB(\n",
       "    (branch3x3): BasicConv2d(\n",
       "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6b): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6c): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6d): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6e): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (AuxLogits): InceptionAux(\n",
       "    (conv0): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv1): BasicConv2d(\n",
       "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       "  (Mixed_7a): InceptionD(\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7b): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7c): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Resnet152\n",
    "resnet152 = models.resnet152(weights=ResNet152_Weights.IMAGENET1K_V1)\n",
    "modules=list(resnet152.children())[:-1]\n",
    "resnet152=nn.Sequential(*modules)\n",
    "for p in resnet152.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "#Efficientnet\n",
    "efficientnet = models.efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "modules = list(efficientnet.children())[:-1]\n",
    "efficientnet_model = nn.Sequential(*modules)\n",
    "for p in efficientnet_model.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "#Inception_v3\n",
    "inception = inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1, transform_input=False)\n",
    "inception.fc = nn.Identity()\n",
    "inception.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones extractores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen las funciones de extracción de características usando varios métodos diferentes: cálculo del histograma, extracción de descriptores SIFT y uso de los modelos de redes neuronales preentrenados ResNet152, EfficientNetB0 e InceptionV3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histo(img):\n",
    "    \"\"\"Convierte la imagen en una matriz plana y calcula un histograma para el rango de valores [0, 256].\n",
    "    Se devuelve como un array de numpy\"\"\"\n",
    "    return np.array([np.histogram(np.asarray(img).ravel(),256,[0,256])[0]]).astype('float32')\n",
    "\n",
    "def sift(img):\n",
    "    \"\"\"Se inicializa un array de 0s con capacidad para almacenar hasta 128 descriptores. \n",
    "    La imagen se convierte a escala de grises  y los descriptores se obtienen \n",
    "    utilizando el operador SIFT previamente definido. \n",
    "    Finalmente, los descriptores generados se almacenan en el array y se devuelven.\"\"\"\n",
    "    xa = np.zeros((1,128)).astype('float32')\n",
    "    sift = cv2.SIFT_create(nfeatures=128)\n",
    "    gray = cv2.cvtColor(np.asarray(img), cv2.COLOR_RGB2GRAY)\n",
    "    _, descriptors = sift.detectAndCompute(gray, mask=None) \n",
    "    a = descriptors[0] \n",
    "    xa[0,:a.size] = a\n",
    "    return xa\n",
    "\n",
    "def resnet(img):\n",
    "    \"\"\"Durante el preprocesado de la imagen, se redimensiona a 256px, se recorta el centro (224px), se convierte\n",
    "    a tensor y se normaliza. Se pasa la imagen al modelo ResNet y se obtiene la salida de 2048 dimensiones.\n",
    "    Este resultado se almacena en un array de numpy y se devuelve.\"\"\"\n",
    "    preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    input_image = img\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    features = resnet152(input_batch).numpy()[0,:,0,0]\n",
    "\n",
    "    a = np.zeros((1,2048)).astype('float32')\n",
    "    a[0,:] = features\n",
    "    return a\n",
    "\n",
    "def efficientnet(img):\n",
    "    \"\"\"Durante el preprocesado de la imagen, se redimensiona a 600px, se convierte\n",
    "    a tensor y se normaliza. Se pasa la imagen al modelo EfficientNet, sin modificar los gradientes,\n",
    "    y se obtiene la salida de 1028 dimensiones.\n",
    "    A este resultado se le aplica otra normalización, se almacena en un array de numpy y se devuelve.\"\"\"\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(600),\n",
    "        transforms.CenterCrop(600),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    input_image = img\n",
    "    input_tensor = preprocess(input_image).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = efficientnet_model(input_tensor).cpu().numpy()[0, :, 0, 0]\n",
    "    features = features / np.linalg.norm(features)\n",
    "\n",
    "    a = np.zeros((1, 1280), dtype=\"float32\")\n",
    "    a[0, :] = features\n",
    "    return a\n",
    "\n",
    "def inception_v3(img):\n",
    "    \"\"\"Durante el preprocesado de la imagen, se redimensiona a 299px, se convierte\n",
    "    a tensor y se normaliza. Se pasa la imagen al modelo InceptionV3, sin modificar los gradientes,\n",
    "    y se obtiene la salida de 2048 dimensiones.\n",
    "    A este resultado se le aplica otra normalización, se almacena en un array de numpy y se devuelve.\"\"\"\n",
    "    preprocess = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_image = img\n",
    "    input_tensor = preprocess(input_image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = inception(input_tensor).cpu().numpy()[0, :]\n",
    "    features = features / np.linalg.norm(features)\n",
    "\n",
    "    a = np.zeros((1, 2048), dtype=\"float32\")\n",
    "    a[0, :] = features\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen las funciones necesarias para poder obtener el accuracy del modelo con cada uno de los extractores. Esto permitirá llegar a una conclusión acerca de cual es el que mejor se adapta al problema planteado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se definen diccionarios para agrupar índices según las clases a las que pertenecen.\n",
    "training_classes = {\n",
    "    'beaches' : list(range(0,90)),\n",
    "    'bus' : list(range(90,180)),\n",
    "    'dinosaurs' : list(range(180,270)),\n",
    "    'elephants' : list(range(270,360)),\n",
    "    'flowers' : list(range(360,450)),\n",
    "    'foods' : list(range(450,540)),\n",
    "    'horses' : list(range(540,630)),\n",
    "    'monuments' : list(range(630,720)),\n",
    "    'mountains_and_snow' : list(range(720,810)),\n",
    "    'people_and_villages_in_Africa' : list(range(810,900)),\n",
    "}\n",
    "\n",
    "test_classes = {\n",
    "    'beaches' : list(range(0,10)),\n",
    "    'bus' : list(range(10,20)),\n",
    "    'dinosaurs' : list(range(20,30)),\n",
    "    'elephants' : list(range(30,40)),\n",
    "    'flowers' : list(range(40,50)),\n",
    "    'foods' : list(range(50,60)),\n",
    "    'horses' : list(range(60,70)),\n",
    "    'monuments' : list(range(70,80)),\n",
    "    'mountains_and_snow' : list(range(80,90)),\n",
    "    'people_and_villages_in_Africa' : list(range(90,100)),\n",
    "}\n",
    "\n",
    "\n",
    "# funciones auxiliares\n",
    "def get_training_class_name(number):\n",
    "    \"\"\"Devuelve el nombre de la clase de entrenamiento asociada a un índice.\"\"\"\n",
    "    for class_name, numbers in training_classes.items():\n",
    "        if number in numbers:\n",
    "            return class_name\n",
    "    return None\n",
    "\n",
    "def get_test_class_name(number):\n",
    "    \"\"\"Devuelve el nombre de la clase de test asociada a un índice.\"\"\"\n",
    "    for class_name, numbers in test_classes.items():\n",
    "        if number in numbers:\n",
    "            return class_name\n",
    "    return None\n",
    "        \n",
    "\n",
    "def get_result_classes(I):\n",
    "    \"\"\"Analiza las clases asociadas a los índices devueltos por una búsqueda. \n",
    "    Calcula un score relativo para cada clase, basado en su frecuencia respecto al total de clases identificadas.\"\"\"\n",
    "    match_classes = [get_training_class_name(i) for i in I]\n",
    "    a = Counter(match_classes)\n",
    "    scores = {}\n",
    "    for k in a.keys():\n",
    "        scores[k] = a[k]/len(match_classes)\n",
    "    return scores\n",
    "    \n",
    "\n",
    "# funciones score\n",
    "def get_extractor_consistency(index_path,k):\n",
    "    \"\"\"Evalúa la consistencia de un extractor de características con base en la diversidad de clases en \n",
    "    los resultados de búsqueda.\"\"\"\n",
    "    index = read_index(index_path)\n",
    "    consistencies = []\n",
    "    for i in range(0,len(col_test)):\n",
    "        if index_path == 'ext_1.index':\n",
    "            a = histo(Image.open(f\"queries/{i}.jpg\"))\n",
    "        \n",
    "        elif index_path == 'ext_2.index':\n",
    "            a = sift(Image.open(f\"queries/{i}.jpg\"))\n",
    "\n",
    "        elif index_path == 'ext_3.index':\n",
    "            a = resnet(Image.open(f\"queries/{i}.jpg\"))\n",
    "        elif index_path == 'ext_4.index':\n",
    "            a = efficientnet(Image.open(f\"queries/{i}.jpg\"))\n",
    "        elif index_path == 'ext_5.index':\n",
    "            a = inception_v3(Image.open(f\"queries/{i}.jpg\"))\n",
    "        else:\n",
    "            a = None\n",
    "        D, I = index.search(a, k)  # Realizar la búsqueda\n",
    "        result_classes = get_result_classes(I[0])\n",
    "        consistencies.append(1/len(result_classes.keys()))\n",
    "    return sum(consistencies) / len(consistencies)\n",
    "\n",
    "def get_acc(i, index_path,k):\n",
    "    \"\"\"Calcula el accuracy para una consulta específica.\"\"\"\n",
    "    index = read_index(index_path)\n",
    "\n",
    "    if index_path == 'ext_1.index':\n",
    "        a = histo(Image.open(f\"queries/{i}.jpg\"))\n",
    "        \n",
    "    elif index_path == 'ext_2.index':\n",
    "        a = sift(Image.open(f\"queries/{i}.jpg\"))\n",
    "\n",
    "    elif index_path == 'ext_3.index':\n",
    "        a = resnet(Image.open(f\"queries/{i}.jpg\"))\n",
    "    elif index_path == 'ext_4.index':\n",
    "        a = efficientnet(Image.open(f\"queries/{i}.jpg\"))\n",
    "    elif index_path == 'ext_5.index':\n",
    "        a = inception_v3(Image.open(f\"queries/{i}.jpg\"))\n",
    "    else:\n",
    "        a = None\n",
    "\n",
    "    _, I = index.search(a, k)\n",
    "    test_class = get_test_class_name(i)\n",
    "    training_score = get_result_classes(I[0])\n",
    "    try:\n",
    "        acc = training_score[test_class]\n",
    "    except:\n",
    "        acc = 0\n",
    "    return training_score, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se evalúan los distintos extractores con las funciones definidas anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluación Histogramas de niveles de gris:\n",
      "Consistencia de las clases resultado: 0.41\n",
      "Accuracy: 0.49\n",
      "-----------------------------------------------------------\n",
      "Evaluación SIFT:\n",
      "Consistencia de las clases resultado: 0.20\n",
      "Accuracy: 0.18\n",
      "-----------------------------------------------------------\n",
      "Evaluación ResNet:\n",
      "Consistencia de las clases resultado: 0.58\n",
      "Accuracy: 0.69\n",
      "-----------------------------------------------------------\n",
      "Evaluación EfficientNet:\n",
      "Consistencia de las clases resultado: 0.38\n",
      "Accuracy: 0.50\n",
      "-----------------------------------------------------------\n",
      "Evaluación Inception V3:\n",
      "Consistencia de las clases resultado: 0.96\n",
      "Accuracy: 0.97\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_faiss_index = {\n",
    "    'Histogramas de niveles de gris': 'ext_1.index', \n",
    "    'SIFT': 'ext_2.index', \n",
    "    'ResNet': 'ext_3.index', \n",
    "    'EfficientNet': 'ext_4.index', \n",
    "    'Inception V3': 'ext_5.index',\n",
    "}\n",
    "\n",
    "for model in model_faiss_index.keys():\n",
    "    faiss_index = model_faiss_index[model]\n",
    "    print(f'Evaluación {model}:')\n",
    "    print(f'Consistencia de las clases resultado: {get_extractor_consistency(faiss_index,10):.2f}')\n",
    "\n",
    "    accs = []\n",
    "    for i in range(0,len(col_test)):\n",
    "        accs.append(get_acc(i,faiss_index,10)[1])\n",
    "    print(f'Accuracy: {(sum(accs) / len(accs)):.2f}')\n",
    "\n",
    "    print('-----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de los extractores y Creación de índices FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACTOR 1 - Histogramas de niveles de gris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraer características: vectores de 256 elementos por imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = np.zeros((len(col),256)).astype('float32')\n",
    "\n",
    "for i in range(0,len(col)):\n",
    "    v = np.histogram(col[i].ravel(),256,[0,256])[0]\n",
    "    xb[i,:] = v\n",
    "\n",
    "#Normalización\n",
    "faiss.normalize_L2(xb)\n",
    "\n",
    "#Índice\n",
    "index = faiss.IndexFlatL2(256)\n",
    "index.add(xb)\n",
    "\n",
    "write_index(index, \"ext_1.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de vectores en el índice: 900\n",
      "Índices de los vecinos más cercanos:\n",
      " [[266 207 189 260 225 217 224 257 183 206 232]]\n",
      "Distancias de los vecinos más cercanos:\n",
      " [[0.01520489 0.01771768 0.01930877 0.01985361 0.02278744 0.02316848\n",
      "  0.02805977 0.03081727 0.03483813 0.03977674 0.03981694]]\n"
     ]
    }
   ],
   "source": [
    "index = read_index(\"ext_1.index\")\n",
    "print(\"Total de vectores en el índice:\", index.ntotal)\n",
    "\n",
    "# Buscar los k vecinos más cercanos\n",
    "k = 11  # Número de vecinos más cercanos que queremos encontrar\n",
    "q_image = Image.open('queries/28.jpg')\n",
    "ax = histo(q_image)\n",
    "faiss.normalize_L2(ax)\n",
    "D, I = index.search(ax, k)  # Realizar la búsqueda\n",
    "\n",
    "print(\"Índices de los vecinos más cercanos:\\n\", I)\n",
    "print(\"Distancias de los vecinos más cercanos:\\n\", D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACTOR 2 - SIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracción de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = []\n",
    "sift_ext = cv2.SIFT_create(nfeatures=128)\n",
    "for i in range(0,len(col)):\n",
    "    gray = cv2.cvtColor(col[i], cv2.COLOR_RGB2GRAY)\n",
    "    keypoints, descriptors = sift_ext.detectAndCompute(gray, mask=None) \n",
    "    b.append(descriptors[0])\n",
    "    \n",
    "xb = np.zeros((len(col),128)).astype('float32')\n",
    "\n",
    "for i in range(0,len(xb)):\n",
    "    xb[i,:b[i].size] = b[i]\n",
    "    \n",
    "#Normalizar\n",
    "faiss.normalize_L2(xb)\n",
    "\n",
    "#índice\n",
    "index = faiss.IndexFlatL2(128)\n",
    "index.add(xb)\n",
    "\n",
    "write_index(index, \"ext_2.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de vectores en el índice: 900\n",
      "Índices de los vecinos más cercanos:\n",
      " [[246 220 798 576 213 190 429 339 793 377 777]]\n",
      "Distancias de los vecinos más cercanos:\n",
      " [[0.13978243 0.19947502 0.22155651 0.25849518 0.28437957 0.30424666\n",
      "  0.31329975 0.3175928  0.34696257 0.35615808 0.3593609 ]]\n"
     ]
    }
   ],
   "source": [
    "index = read_index(\"ext_2.index\")\n",
    "print(\"Total de vectores en el índice:\", index.ntotal)\n",
    "\n",
    "# Buscar los k vecinos más cercanos\n",
    "k = 11  # Número de vecinos más cercanos que queremos encontrar\n",
    "q_image = Image.open('queries/28.jpg')\n",
    "xa = sift(q_image)\n",
    "faiss.normalize_L2(xa)\n",
    "D, I = index.search(xa, k)  # Realizar la búsqueda\n",
    "\n",
    "print(\"Índices de los vecinos más cercanos:\\n\", I)\n",
    "print(\"Distancias de los vecinos más cercanos:\\n\", D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACTOR 3 - ResNet preentrenada sin la última capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracción de características. Vector de 2048 elementos por imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_resnet(index):\n",
    "    input_image = Image.open(f\"images/{index}.jpg\")\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    features = resnet152(input_batch)\n",
    "    return features.numpy()[0,:,0,0]\n",
    "\n",
    "\n",
    "xb = np.zeros((900,2048)).astype('float32')\n",
    "\n",
    "for i in range(0,len(xb)):\n",
    "    xb[i,:] = features_resnet(i)\n",
    "    \n",
    "#Normalización\n",
    "faiss.normalize_L2(xb)\n",
    "\n",
    "#Creación del índice\n",
    "index = faiss.IndexFlatL2(2048)\n",
    "index.add(xb)\n",
    "\n",
    "write_index(index, \"ext_3.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de vectores en el índice: 900\n",
      "Índices de los vecinos más cercanos:\n",
      " [[219 196 192 227]]\n",
      "Distancias de los vecinos más cercanos:\n",
      " [[0.00828474 0.00840403 0.00849715 0.00855546]]\n"
     ]
    }
   ],
   "source": [
    "index = read_index(\"ext_3.index\")\n",
    "print(\"Total de vectores en el índice:\", index.ntotal)\n",
    "\n",
    "# Buscar los k vecinos más cercanos\n",
    "k = 4  # Número de vecinos más cercanos que queremos encontrar\n",
    "q_image = Image.open('queries/28.jpg')\n",
    "ax = resnet(q_image)\n",
    "faiss.normalize_L2(ax)\n",
    "D, I = index.search(ax, k)  # Realizar la búsqueda\n",
    "\n",
    "print(\"Índices de los vecinos más cercanos:\\n\", I)\n",
    "print(\"Distancias de los vecinos más cercanos:\\n\", D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACTOR 4 - EfficientNet preentrenada sin la última capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_efficientnet(index):\n",
    "    input_image = Image.open(f\"images/{index}.jpg\")\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(600),\n",
    "        transforms.CenterCrop(600),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = preprocess(input_image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = efficientnet_model(input_tensor).cpu().numpy()[0, :, 0, 0]\n",
    "    return features / np.linalg.norm(features)\n",
    "\n",
    "xa = np.zeros((900, 1280), dtype=\"float32\")\n",
    "\n",
    "for i in range(0, len(xa)):\n",
    "    xa[i, :] = features_efficientnet(i)\n",
    "    \n",
    "#Normalización\n",
    "faiss.normalize_L2(xa)\n",
    "\n",
    "#Creacción del índice\n",
    "index = faiss.IndexFlatL2(1280)\n",
    "index.add(xa)\n",
    "\n",
    "write_index(index, \"ext_4.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de vectores en el índice: 900\n",
      "Índices de los vecinos más cercanos:\n",
      " [[225 690 693 198]]\n",
      "Distancias de los vecinos más cercanos:\n",
      " [[0.05610025 0.06011028 0.06036519 0.0605374 ]]\n"
     ]
    }
   ],
   "source": [
    "index = read_index(\"ext_4.index\")\n",
    "\n",
    "print(\"Total de vectores en el índice:\", index.ntotal)\n",
    "k = 4\n",
    "q_image = Image.open('queries/28.jpg')\n",
    "ax = efficientnet(q_image)\n",
    "faiss.normalize_L2(ax)\n",
    "D, I = index.search(ax, k)\n",
    "\n",
    "print(\"Índices de los vecinos más cercanos:\\n\", I)\n",
    "print(\"Distancias de los vecinos más cercanos:\\n\", D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACTOR 5 - Inception V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def extract_features(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    input_tensor = preprocess(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = inception(input_tensor).numpy()\n",
    "    return features.flatten()\n",
    "\n",
    "#En este caso se crea una matriz de características normalizada\n",
    "def build_feature_matrix(image_folder, num_images):\n",
    "    feature_matrix = np.zeros((num_images, 2048), dtype=\"float32\")\n",
    "    for i in range(num_images):\n",
    "        image_path = f\"{image_folder}/{i}.jpg\"\n",
    "        feature_matrix[i, :] = extract_features(image_path)\n",
    "    faiss.normalize_L2(feature_matrix)\n",
    "    return feature_matrix\n",
    "\n",
    "image_folder = \"images\"\n",
    "num_images = 900\n",
    "features = build_feature_matrix(image_folder, num_images)\n",
    "\n",
    "#Creacción del índice\n",
    "def create_faiss_index(features):\n",
    "    index = faiss.IndexFlatL2(features.shape[1])\n",
    "    index.add(features)\n",
    "    return index\n",
    "\n",
    "def search_faiss_index(index, query_feature, k=4):\n",
    "    faiss.normalize_L2(query_feature)\n",
    "    distances, indices = index.search(query_feature, k)\n",
    "    return distances, indices\n",
    "\n",
    "index = create_faiss_index(features)\n",
    "write_index(index, \"ext_5.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de vectores en el índice: 900\n",
      "Índices de los vecinos más cercanos: [[192 225 219 240]]\n",
      "Distancias de los vecinos más cercanos: [[0.22372064 0.2951452  0.3097215  0.3203855 ]]\n"
     ]
    }
   ],
   "source": [
    "index = read_index(\"ext_5.index\")\n",
    "print(f\"Total de vectores en el índice: {index.ntotal}\")\n",
    "\n",
    "query_image_path = 'queries/28.jpg'\n",
    "query_feature = extract_features(query_image_path).reshape(1, -1).astype(\"float32\")\n",
    "k = 4\n",
    "distances, indices = search_faiss_index(index, query_feature, k)\n",
    "\n",
    "print(\"Índices de los vecinos más cercanos:\", indices)\n",
    "print(\"Distancias de los vecinos más cercanos:\", distances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
